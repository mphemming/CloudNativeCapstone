{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428da3f5",
   "metadata": {},
   "source": [
    "# Cloud Native Geospatial Capstone Project\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "### Summary\n",
    "\n",
    "Create a file collection summary (e.g. number of datapoints / deployments)  by applying SQL commands to any parquet data set stored in `aodn-cloud-optimised` using DuckDB. (I use dask and S3 as well) \n",
    "\n",
    "### Test collection\n",
    "\n",
    "slocum_glider_delayed_qc.parquet/ -> 553 objects, ~34.2GB \n",
    "\n",
    "### Motivation\n",
    "\n",
    "Using familiar tools such as xarray and pandas are slow!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62935ee",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import dask.dataframe as dd\n",
    "import s3fs\n",
    "import sys\n",
    "import os\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# for timing the notebook execution\n",
    "notebook_start = time.perf_counter()\n",
    "\n",
    "# load AWS profile name\n",
    "with open(\"AWS.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    aws_profile = f.read().strip()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70133779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name and S3 URI\n",
    "\n",
    "dataset_name = 'vessel_fishsoop_realtime_qc'\n",
    "s3_uri = f's3://aodn-cloud-optimised/{dataset_name}.parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d332b4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aodn-cloud-optimised/animal_acoustic_tracking_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/animal_ctd_satellite_relay_tagging_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/argo.parquet',\n",
       " 'aodn-cloud-optimised/autonomous_underwater_vehicle.parquet',\n",
       " 'aodn-cloud-optimised/diver_benthic_cover_in_situ_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_cryptobenthic_fish_abundance_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_mobile_macroinvertebrate_abundance_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_off_transect_species_observations_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_reef_fish_abundance_biomass_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_site_information_qc.parquet',\n",
       " 'aodn-cloud-optimised/diver_survey_metadata_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_acidification_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_acidification_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_ctd_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_hourly_timeseries_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_satellite_altimetry_calibration_validation.parquet',\n",
       " 'aodn-cloud-optimised/mooring_southern_ocean_surface_fluxes_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_southern_ocean_surface_properties_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_temperature_logger_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/mooring_timeseries_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/slocum_glider_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/station_nrs_wireless_sensor_network_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/station_wireless_sensor_network_delayec_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_air_sea_flux_product_delayed.parquet',\n",
       " 'aodn-cloud-optimised/vessel_air_sea_flux_sst_meteo_realtime.parquet',\n",
       " 'aodn-cloud-optimised/vessel_co2_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_fishsoop_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_nrs_ctd_profiles_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_sst_delayed_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_sst_realtime_nonqc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_trv_realtime_qc.parquet',\n",
       " 'aodn-cloud-optimised/vessel_xbt_realtime_nonqc.parquet',\n",
       " 'aodn-cloud-optimised/wave_buoy_realtime_nonqc.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all parquet datasets on S3\n",
    "\n",
    "def list_parquet_files():\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    files = fs.glob(f's3://aodn-cloud-optimised/*.parquet')\n",
    "    return files\n",
    "\n",
    "available_parquet_datasets = list_parquet_files()\n",
    "available_parquet_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c2024",
   "metadata": {},
   "source": [
    "### Get a summary of the data set using S3 and dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get s3 size of dataset and number of files\n",
    "def get_s3_size_and_file_count(s3_uri):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    files = fs.glob(s3_uri + '**/*.parquet')\n",
    "\n",
    "    for file in files:\n",
    "        total_size += fs.info(file)['Size']\n",
    "        file_count += 1\n",
    "        \n",
    "    print(f\"Total size of dataset: {total_size / (1024**3):.2f} GB\")\n",
    "    print(f\"Total number of files: {file_count}\")\n",
    "\n",
    "    return total_size, file_count, files\n",
    "\n",
    "_,_,files = get_s3_size_and_file_count(s3_uri)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract NetCDF files appended to the parquet dataset\n",
    "\n",
    "nc_files = [\n",
    "    os.path.basename(p).split(\".nc\")[0] + \".nc\"\n",
    "    for p in files\n",
    "]\n",
    "nc_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ec390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date the parquet dataset was created and last updated\n",
    "# Need to login via SSO\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "    \n",
    "latest = None\n",
    "first = None\n",
    "\n",
    "for path in fs.find(s3_uri):\n",
    "    info = fs.info(path)\n",
    "    modified = info[\"LastModified\"]\n",
    "    \n",
    "    if latest is None or modified > latest:\n",
    "        latest = modified\n",
    "    if first is None or modified < first:\n",
    "        first = modified\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ea419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_ds(s3_uri):\n",
    "    # use dask to read the parquet file from S3\n",
    "    ddf = dd.read_parquet(s3_uri, engine='pyarrow',\n",
    "                        storage_options={\"profile\": aws_profile})\n",
    "    for col, dtype in ddf.dtypes.items():\n",
    "        print(f\"{col:<35} {dtype}\")\n",
    "    \n",
    "    # summarise partitions\n",
    "    print(' ' * 150)\n",
    "    print(f\"This data set has {ddf.npartitions} partitions.\")\n",
    "    print(' ' * 150)\n",
    "    fs = s3fs.S3FileSystem(profile=aws_profile)\n",
    "    partitions = fs.ls(s3_uri)\n",
    "    partition_names = [p.split(\"=\")[-1] for p in partitions]\n",
    "    # get partition and non-partition columns\n",
    "    partition_column_names = list(\n",
    "        ddf.select_dtypes(include=\"category\").columns\n",
    "    )\n",
    "    column_names = list(\n",
    "        ddf.select_dtypes(exclude=\"category\").columns\n",
    "    )\n",
    "    print(f\"Partition column names: {', '.join(partition_column_names)}\")\n",
    "\n",
    "    return partition_names, partition_column_names, column_names  \n",
    "\n",
    "\n",
    "partition_names, partition_column_names, column_names = summarise_ds(s3_uri)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3b0a4",
   "metadata": {},
   "source": [
    "### Use DuckDB to get information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DuckDB connection and setup\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"\"\"\n",
    "            SET s3_region='ap-southeast-2';\n",
    "            \"\"\") \n",
    "con.execute(\"PRAGMA enable_progress_bar\") # show progress bar for long-running queries\n",
    "con.execute(\"PRAGMA threads=14\") # use all available threads for query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d63fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all parquet files in the dataset via DuckDB\n",
    "paths = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        file\n",
    "    FROM glob('{s3_uri}**/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "paths[\"file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SQL query for DuckDB\n",
    "\n",
    "partition_names_sql = \", \".join(f\"'{d}'\" for d in partition_names[1::]) # skip the first partition which is the metadata file\n",
    "\n",
    "# grab the first partition column name (e.g. deployment_code for slocum underwater gliders)\n",
    "partition_column_name = partition_column_names[0]\n",
    "\n",
    "# If the dataset has TIME, LONGITUDE and LATITUDE columns, assume it's an IMOS dataset and get more detailed summary statistics.\n",
    "# Otherwise, just get row counts by partition.\n",
    "if 'TIME' in column_names and 'LONGITUDE' in column_names and 'LATITUDE' in column_names:\n",
    "\n",
    "    # This is an IMOS dataset (e.g. slocum_glider_delayed_qc)\n",
    "    # Variable names are standardised\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "    {partition_column_name},\n",
    "    MIN(TIME) AS start_time,\n",
    "    MAX(TIME) AS end_time,\n",
    "    MIN(LONGITUDE) AS min_longitude,\n",
    "    MAX(LONGITUDE) AS max_longitude,\n",
    "    MIN(LATITUDE) AS min_latitude,\n",
    "    MAX(LATITUDE) AS max_latitude,\n",
    "    COUNT(*)  AS n_rows\n",
    "    FROM read_parquet(\n",
    "    '{s3_uri}**/*.parquet',\n",
    "    hive_partitioning=1\n",
    "    )\n",
    "    WHERE {partition_column_name} IN ({partition_names_sql})\n",
    "    GROUP BY {partition_column_name}\n",
    "    ORDER BY {partition_column_name}\n",
    "    \"\"\"\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # This is not an IMOS dataset, so just get row counts by partition\n",
    "    # Variable names will likely vary by dataset\n",
    "    # (e.g. argo)\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "    {partition_column_name},\n",
    "    COUNT(*)  AS n_rows\n",
    "    FROM read_parquet(\n",
    "    '{s3_uri}**/*.parquet',\n",
    "    hive_partitioning=1\n",
    "    )\n",
    "    WHERE {partition_column_name} IN ({partition_names_sql})\n",
    "    GROUP BY {partition_column_name}\n",
    "    ORDER BY {partition_column_name}\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d10783",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = con.execute(sql).fetchdf()\n",
    "\n",
    "# Testing \n",
    "# hive partitioning, LIMIT 1, group/order by deployment code, count rows, min/max time = 6m45s\n",
    "# Specifying a list of partition names decreased time to ~5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'start_time' in stats.columns and 'end_time' in stats.columns:\n",
    "    # Format start_time and end_time as strings for better display in HTML table\n",
    "    stats[\"start_time\"] = stats[\"start_time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    stats[\"end_time\"]   = stats[\"end_time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f330087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of rows across all partitions\n",
    "total_rows = stats[\"n_rows\"].sum()\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f891ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to plot the approximate locations of the partitions based on their min latitude and longitude.\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "# Set Australia extent\n",
    "ax.set_extent([110, 155, -45, -10], crs=ccrs.PlateCarree())\n",
    "# Add coastline\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "if 'min_latitude' in stats.columns and 'min_longitude' in stats.columns:\n",
    "    # Scatter\n",
    "    ax.scatter(\n",
    "        stats['min_longitude'],\n",
    "        stats['min_latitude'],\n",
    "        s=stats['n_rows'] / 300,\n",
    "        edgecolors='k',\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "# Set title\n",
    "ax.set_title(\"Partition approx locations (Australia)\")\n",
    "\n",
    "# Save figure to memory buffer (to include in the html summary)\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "buf.seek(0)\n",
    "# Convert to base64\n",
    "img_base64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fcc2f",
   "metadata": {},
   "source": [
    "### Export information to a html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f875f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_prints_to_html(func, dataset_name, output_file='output.html', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Execute a function and capture all print statements to an HTML file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    func : callable\n",
    "        The function to execute\n",
    "    dataset_name : str\n",
    "        Name of the dataset\n",
    "    output_file : str\n",
    "        Path to the output HTML file\n",
    "    *args, **kwargs\n",
    "        Arguments to pass to the function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : any\n",
    "    func : callable\n",
    "        The function to execute\n",
    "    output_file : str\n",
    "        Path to the output HTML file\n",
    "    *args, **kwargs\n",
    "        Arguments to pass to the function\n",
    "        }}\n",
    "        .container {{\n",
    "            background-color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #333;\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : any\n",
    "        The return value of the function\n",
    "    \"\"\"\n",
    "    # Create a StringIO object to capture stdout\n",
    "    captured_output = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    \n",
    "    try:\n",
    "        # Redirect stdout to our StringIO object\n",
    "        sys.stdout = captured_output\n",
    "        \n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "    finally:\n",
    "        # Restore stdout\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    # Get the captured output\n",
    "    output_text = captured_output.getvalue()\n",
    "    \n",
    "    # Create HTML content\n",
    "    html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{dataset_name}</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Courier New', monospace;\n",
    "            font-size: 16px;\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 20px;\n",
    "            max-width: 95vw;\n",
    "            margin: 0 auto;\n",
    "        }}\n",
    "        .container {{\n",
    "            background-color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #333;\n",
    "            font-size: 28px;\n",
    "            border-bottom: 2px solid #4CAF50;\n",
    "            padding-bottom: 10px;\n",
    "        }}\n",
    "        .timestamp {{\n",
    "            color: #666;\n",
    "            font-size: 14px;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "        pre {{\n",
    "            background-color: #f8f8f8;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 4px;\n",
    "            padding: 15px;\n",
    "            overflow-x: auto;\n",
    "            line-height: 1.6;\n",
    "            font-size: 15px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>{dataset_name}</h1>\n",
    "        <div class=\"timestamp\">Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>\n",
    "        <pre>{output_text}</pre>\n",
    "        <img src=\"data:image/png;base64,{img_base64}\" style=\"max-width:100%; height:auto;\" />\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"Output saved to: {output_file}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac067c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title(title_str):\n",
    "    print(\"=\" * 150)\n",
    "    print(title_str.center(150))\n",
    "    print(\"=\" * 150)\n",
    "    print(' ' * 150)\n",
    "\n",
    "\n",
    "def html_summary_content():\n",
    "    \n",
    "    # S3 summaryTotal notebook runtimep is much faste\n",
    "    add_title(\"PARQUET FILE SUMMARY\")\n",
    "    _,_,files = get_s3_size_and_file_count(s3_uri)\n",
    "    print(' ' * 150)\n",
    "    print(f\"Parquet file created: {first}\")\n",
    "    print(f\"Parquet file last updated: {latest}\")\n",
    "    print(' ' * 150)\n",
    "\n",
    "    # List the files in the dataset\n",
    "    add_title(\"DATASET VARIABLES AND PARTITIONS\")\n",
    "    summarise_ds(s3_uri)\n",
    "    print(' ' * 150)\n",
    "    \n",
    "    # Deployment stats\n",
    "    add_title(\"DATASET STATISTICS\")\n",
    "    print(f\"\\nTotal rows: {total_rows}\")\n",
    "    print(' ' * 150)\n",
    "    print(stats.to_string())\n",
    "    print(' ' * 150)\n",
    "    \n",
    "    # List the files in the dataset\n",
    "    add_title(\"PARQUET FILES IN DATASET\")\n",
    "    print(' ' * 150)\n",
    "    for file in files:\n",
    "        print(file)\n",
    "    print(\"=\" * 150)\n",
    "    print(' ' * 150)\n",
    "    add_title(\"NETCDF FILES APPENDED\")\n",
    "    print(' ' * 150)\n",
    "    for file in nc_files:\n",
    "        print(file)\n",
    "    print(\"=\" * 150)\n",
    "    print(' ' * 150)\n",
    "\n",
    "    # work out total notebook runtime\n",
    "    notebook_end = time.perf_counter()\n",
    "    total_elapsed = notebook_end - notebook_start\n",
    "    minutes = int(total_elapsed // 60)\n",
    "    seconds = total_elapsed % 60\n",
    "    print(' ' * 150)\n",
    "    print(f\"Total notebook runtime: {minutes}m {seconds:.1f}s\")\n",
    "    print(' ' * 150)\n",
    "    \n",
    "\n",
    "# Capture all output to a single HTML file\n",
    "capture_prints_to_html(html_summary_content, dataset_name, f'{dataset_name}_summary.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c29d4",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fa7b7",
   "metadata": {},
   "source": [
    "- The use of the DuckDB progress bar was a quick way to see if my query was reasonable (if it would take too long)\n",
    "\n",
    "- There are two progress bars when loading into memory (first one for the DuckDB query, and second one for memory (much longer))\n",
    "- it would estimate a longer period of time to execute, and then suddenly finish (e.g. 35 min at first, took 18 mins)\n",
    "- Earlier testing suggests using duckdb with 14 cores makes querying twice as fast\n",
    "- Setting hive partitioning to True and enabling partition pruning is much faster for getting statistics - perhaps because DuckDB is told where to look rather than discovering partitions/loading metadata?\n",
    "- Multiple ways to get partition information\n",
    "- Even though `slocum_glider_delayed_qc.parquet` is much larger (~34.2GB) compared to `argo.parquet` (~3GB), the notebook required more time to complete, suggesting that we need to optimise this file. Other similar sized datasets took ~7mins instead of ~25mins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707254c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriveGeo (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
